{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('HerMajestySpeechesDataset/train.txt', 'r', encoding='utf-8')\n",
    "# text = f.read()\n",
    "texts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2361"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts as IDs: [[2, 181, 999, 5, 39, 584, 41, 2094, 2095, 21, 1165, 530], [21, 9, 163, 246, 8, 1000, 25, 424, 1166, 5, 18, 20], [68, 195, 781], [2, 355, 3, 7, 1347, 722, 53, 585, 40, 213, 2, 297, 3, 531, 208, 1001], [2, 181, 68, 195, 6, 781, 2096, 21, 532, 117, 872, 4, 247], [2, 195, 49, 2097, 6, 2951, 1167, 6, 151, 224, 196, 491, 533, 2098, 197, 21, 7, 2952, 2953], [8, 13, 1168, 205, 873, 5, 114, 10, 455, 187, 298, 586, 2, 259, 11, 111, 205], [58, 3, 2, 455, 10, 29, 50, 649, 20, 10, 79, 11, 66, 12, 2, 312, 3, 68], [17, 271, 7, 59, 83, 8, 782, 5, 456, 2099, 2, 2954, 3, 2, 356, 4, 214, 10, 874, 49, 492, 5, 237, 129, 4, 493, 5, 7, 875, 42], [2, 355, 3, 783, 2100, 2955, 16, 457, 5, 20, 43, 492, 248, 4, 91, 2, 69, 2956, 11, 141]]\n",
      "IDs back to texts: [\"the queen's messages to those celebrating their 90th birthdays on 21 april\", 'on this shared occasion i send my warm congratulations to you all', 'christmas broadcast 2006', 'the birth of a baby brings great happiness but then the business of growing up begins', \"the queen's christmas broadcast in 2006 focused on understanding between faiths and generations\", 'the broadcast was filmed in southwark cathedral in london where her majesty met schoolchildren working on a nativity collage', 'i have lived long enough to know that things never remain quite the same for very long', 'one of the things that has not changed all that much for me is the celebration of christmas', 'it remains a time when i try to put aside the anxieties of the moment and remember that christ was born to bring peace and tolerance to a troubled world', 'the birth of jesus naturally turns our thoughts to all new born children and what the future holds for them']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(texts) \n",
    "\n",
    "texts2ids = tokenizer.texts_to_sequences(texts)\n",
    "print (\"Texts as IDs:\", texts2ids[:10])\n",
    "ids2texts = tokenizer.sequences_to_texts(texts2ids)\n",
    "print (\"IDs back to texts:\", ids2texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams(frase, size=2):\n",
    "    for i in range(size):\n",
    "        frase = [0] + frase\n",
    "    ngrams_list = []\n",
    "    for i in range(len(frase)-size):\n",
    "        ngrams_list.append((tuple(frase[i:i+size]),frase[i+size]))\n",
    "    return ngrams_list\n",
    "\n",
    "all_ngrams = []\n",
    "for i in texts2ids:\n",
    "    all_ngrams += get_ngrams(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def co_table(lista_ocurrencias):\n",
    "    table = {}\n",
    "    for i in lista_ocurrencias:\n",
    "        if i[0] in table:\n",
    "            if i[1] in table[i[0]]:\n",
    "                table[i[0]][i[1]] += 1\n",
    "            else:\n",
    "                table[i[0]][i[1]] = 1\n",
    "        else:\n",
    "            table[i[0]] = {}\n",
    "            table[i[0]][i[1]] = 1\n",
    "    return table\n",
    "\n",
    "table = co_table(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_toyLM_ngram_a(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for _ in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            return tokenizer.sequences_to_texts([cadena])\n",
    "        else:\n",
    "            new = max(table[context], key=table[context].get)\n",
    "            cadena.append(new)\n",
    "    return tokenizer.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_ngram_a(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_toyLM_ngram_b(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for i in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            return tokenizer.sequences_to_texts([cadena])\n",
    "        else:\n",
    "            lista_tmp = list()\n",
    "            for i in table[context]:\n",
    "                for j in range(table[context][i]):\n",
    "                    lista_tmp.append(i)\n",
    "            new = np.random.randint(len(lista_tmp))\n",
    "            cadena.append(lista_tmp[new])\n",
    "    return tokenizer.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_ngram_b(table, (424, 1166))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toyLM_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(text, max_seq_length=10):\n",
    "    train_set = dict()\n",
    "    for frase in text:\n",
    "        for word_index in range(len(frase)):\n",
    "            if word_index < max_seq_length:\n",
    "                train_set[tuple(pad_sequences([frase[:word_index]], maxlen=max_seq_length)[0])] = frase[word_index]\n",
    "    return train_set\n",
    "    \n",
    "train_set = train_generate(texts2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(list(train_set.keys()))\n",
    "y_train = to_categorical(list(train_set.values()), num_classes=len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "502/502 [==============================] - 51s 10ms/step - loss: 6.8330 - accuracy: 0.0585\n",
      "Epoch 2/10\n",
      "502/502 [==============================] - 5s 11ms/step - loss: 6.2733 - accuracy: 0.0639\n",
      "Epoch 3/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 6.1494 - accuracy: 0.0814\n",
      "Epoch 4/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 6.0370 - accuracy: 0.0864\n",
      "Epoch 5/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 5.9362 - accuracy: 0.0950\n",
      "Epoch 6/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 5.8380 - accuracy: 0.0984\n",
      "Epoch 7/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 5.7258 - accuracy: 0.1048\n",
      "Epoch 8/10\n",
      "502/502 [==============================] - 5s 9ms/step - loss: 5.6050 - accuracy: 0.1146\n",
      "Epoch 9/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 5.4871 - accuracy: 0.1208\n",
      "Epoch 10/10\n",
      "502/502 [==============================] - 5s 10ms/step - loss: 5.3725 - accuracy: 0.1267\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(5614, 20, input_length=10),\n",
    "    LSTM(64),\n",
    "    Dense(5614, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaf8493770d7c647504f8822272130844a080041caffea46428413e90a69177e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('ple': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
