{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDatasets(path):\n",
    "    f = open(path, 'r', encoding='utf-8')\n",
    "    texts = f.readlines()\n",
    "    f.close()\n",
    "    return texts\n",
    "\n",
    "text_train = readDatasets('HerMajestySpeechesDataset/train.txt')\n",
    "text_test = readDatasets('HerMajestySpeechesDataset/test.txt')\n",
    "text_val = readDatasets('HerMajestySpeechesDataset/dev.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_train = Tokenizer(oov_token='<unk>', num_words = 2000)\n",
    "tokenizer_train.fit_on_texts(text_train) \n",
    "texts2ids_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "\n",
    "texts2ids_test = tokenizer_train.texts_to_sequences(text_test)\n",
    "\n",
    "texts2ids_val = tokenizer_train.texts_to_sequences(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.concatenate(texts2ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 8 13\n"
     ]
    }
   ],
   "source": [
    "print(len(texts2ids_train[0]), len(texts2ids_test[0]), len(texts2ids_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in texts2ids_train:\n",
    "    i.append(2000)\n",
    "\n",
    "for i in texts2ids_test:\n",
    "    i.append(2000)\n",
    "\n",
    "for i in texts2ids_val:\n",
    "    i.append(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 9 14\n"
     ]
    }
   ],
   "source": [
    "print(len(texts2ids_train[0]), len(texts2ids_test[0]), len(texts2ids_val[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams(frase, size=2):\n",
    "    frase = np.concatenate((np.zeros(size), frase))\n",
    "\n",
    "    ngrams_list = []\n",
    "    for i in range(len(frase)-size):\n",
    "        ngrams_list.append((tuple(frase[i:i+size]),frase[i+size]))\n",
    "    return ngrams_list\n",
    "\n",
    "all_ngrams = []\n",
    "for i in texts2ids_train:\n",
    "    all_ngrams += get_ngrams(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def co_table(lista_ocurrencias):\n",
    "    table = {}\n",
    "    for i in lista_ocurrencias:\n",
    "        if i[0] in table:\n",
    "            if i[1] in table[i[0]]:\n",
    "                table[i[0]][i[1]] += 1\n",
    "            else:\n",
    "                table[i[0]][i[1]] = 1\n",
    "        else:\n",
    "            table[i[0]] = {}\n",
    "            table[i[0]][i[1]] = 1\n",
    "    return table\n",
    "\n",
    "table = co_table(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glasgow when over seventy nations and territories are gathered here in the <unk> of the']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toyLM_ngram_a(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for _ in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            break\n",
    "        else:\n",
    "            new = max(table[context], key=table[context].get)\n",
    "            if new == 2000:\n",
    "                break\n",
    "            cadena.append(new)\n",
    "    return tokenizer_train.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_ngram_a(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perplexity_ngrams(frases):\n",
    "    tmp = []\n",
    "    for frase in frases:\n",
    "        n_grams = get_ngrams(frase)\n",
    "        perplexity = 1\n",
    "        for context, following in n_grams:\n",
    "            if context in table.keys():\n",
    "                denominador = sum(table[context].values())\n",
    "                if following in table[context].keys():\n",
    "                    numerador = table[context][following]\n",
    "                else:\n",
    "                    numerador = 0\n",
    "            else:\n",
    "                denominador, numerador = 0, 0\n",
    "\n",
    "            numerador += 1\n",
    "            denominador += 2000\n",
    "\n",
    "            perplexity *= 1/(numerador/denominador)\n",
    "        tmp.append(perplexity**(1/len(n_grams)))\n",
    "\n",
    "    return np.mean(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992.1306822885472"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_ngrams(texts2ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in holyrood you have the faith to ask <unk>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toyLM_ngram_b(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for i in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            break\n",
    "        else:\n",
    "            lista_tmp = list()\n",
    "            for i in table[context]:\n",
    "                for j in range(table[context][i]):\n",
    "                    lista_tmp.append(i)\n",
    "            new = np.random.randint(len(lista_tmp))\n",
    "            if new == 2000:\n",
    "                break\n",
    "            cadena.append(lista_tmp[new])\n",
    "    return tokenizer_train.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_ngram_b(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# toyLM_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(text, max_seq_length=10):\n",
    "    train_set = dict()\n",
    "    for frase in text:\n",
    "        for word_index in range(len(frase)):\n",
    "            if word_index < max_seq_length:\n",
    "                train_set[tuple(pad_sequences([frase[:word_index]], maxlen=max_seq_length)[0])] = frase[word_index]\n",
    "    return train_set\n",
    "    \n",
    "train_set = train_generate(texts2ids_train)\n",
    "test_set = train_generate(texts2ids_test)\n",
    "val_set = train_generate(texts2ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_generate(text, size=2):\n",
    "    x = []\n",
    "    y = []\n",
    "    for phrase in text:\n",
    "        for context, following in get_ngrams(phrase,size):\n",
    "            x.append(list(context))\n",
    "            y.append(following)\n",
    "    return np.array(x), to_categorical(np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12971, 2) (12971, 2001)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = train_generate(texts2ids_train, 2)\n",
    "x_test, y_test = train_generate(texts2ids_test, 2)\n",
    "x_val, y_val = train_generate(texts2ids_val, 2)\n",
    "\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   2],\n",
       "       [  0,   0,   0, ...,   0,   2, 181],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   5,   2, 123],\n",
       "       [  0,   0, 132, ...,   2, 123, 168],\n",
       "       [  0, 132, 131, ..., 123, 168,   4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(list(train_set.keys()))\n",
    "x_train\n",
    "# y_train = to_categorical(list(train_set.values()), num_classes=len(tokenizer_train.word_index))\n",
    "#\n",
    "# x_test = np.array(list(test_set.keys()))\n",
    "# y_test = to_categorical(list(test_set.values()), num_classes=len(tokenizer_test.word_index))\n",
    "#\n",
    "# x_val = np.array(list(val_set.keys()))\n",
    "# y_val = to_categorical(list(val_set.values()), num_classes=len(tokenizer_val.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 16043\n  y sizes: 55078\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PLE\\Repositorio PLN\\main.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=1'>2</a>\u001b[0m     Embedding(\u001b[39m2001\u001b[39m, \u001b[39m20\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=2'>3</a>\u001b[0m     LSTM(\u001b[39m64\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=3'>4</a>\u001b[0m     Dense(\u001b[39m2001\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=4'>5</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000018?line=8'>9</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PIVA\\P1\\piva\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PIVA\\P1\\piva\\lib\\site-packages\\keras\\engine\\data_adapter.py:1653\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/engine/data_adapter.py?line=1648'>1649</a>\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/engine/data_adapter.py?line=1649'>1650</a>\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/engine/data_adapter.py?line=1650'>1651</a>\u001b[0m                        \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/engine/data_adapter.py?line=1651'>1652</a>\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Peter/Desktop/Q6/PIVA/P1/piva/lib/site-packages/keras/engine/data_adapter.py?line=1652'>1653</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 16043\n  y sizes: 55078\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(2001, 20),\n",
    "    LSTM(64),\n",
    "    Dense(2001, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perplexity_lstm(model, frases):\n",
    "    tmp = []\n",
    "    for frase in frases:\n",
    "        n_grams = get_ngrams(frase)\n",
    "        perplexity = 1\n",
    "        for context, following in n_grams:\n",
    "            prob = model.predict(np.array([context]))\n",
    "\n",
    "            perplexity *= 1/prob[0][int(following)]\n",
    "        tmp.append(perplexity**(1/len(n_grams)))\n",
    "\n",
    "    return np.mean(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.3956923574683"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_lstm(model, texts2ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line <unk> tenth legislation constant officers answer fashion october fifth fifth wider fifth already far']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toyLM_lstm_a(model, context='aleatorio', n=15):\n",
    "    if context=='aleatorio':\n",
    "        context = list(np.random.randint(0, 5616,2))\n",
    "\n",
    "    cadena = context\n",
    "    for _ in range(n-len(context)):\n",
    "        context = cadena[-len(context):]\n",
    "        new = np.argmax(model.predict(np.array([context])))\n",
    "        if new == 5615:\n",
    "            break\n",
    "        cadena.append(new)\n",
    "    return tokenizer_train.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_lstm_a(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['given memory in the commonwealth and views that in glasgow africa is our symbol of']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toyLM_lstm_b(model, context='aleatorio', n=15):\n",
    "    if context=='aleatorio':\n",
    "        context = list(np.random.randint(0, 2001,2))\n",
    "\n",
    "    cadena = context\n",
    "    for _ in range(n-len(context)):\n",
    "        context = cadena[-len(context):]\n",
    "        probs = model.predict(np.array([context]))\n",
    "        new = np.random.choice(range(2001), p=probs[0])\n",
    "        if new == 5615:\n",
    "            break\n",
    "        cadena.append(new)\n",
    "    return tokenizer_train.sequences_to_texts([cadena])\n",
    "\n",
    "generate_toyLM_lstm_b(model)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67363ff55afc029ab1fddd3e5e5f9b2a2972c16f21ded0c1fb15c2e34e2e8559"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('piva': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
