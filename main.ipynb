{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('HerMajestySpeechesDataset/train.txt', 'r', encoding='utf-8')\n",
    "# text = f.read()\n",
    "texts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2361"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts as IDs: [[2, 181, 999, 5, 39, 584, 41, 2094, 2095, 21, 1165, 530], [21, 9, 163, 246, 8, 1000, 25, 424, 1166, 5, 18, 20], [68, 195, 781], [2, 355, 3, 7, 1347, 722, 53, 585, 40, 213, 2, 297, 3, 531, 208, 1001], [2, 181, 68, 195, 6, 781, 2096, 21, 532, 117, 872, 4, 247], [2, 195, 49, 2097, 6, 2951, 1167, 6, 151, 224, 196, 491, 533, 2098, 197, 21, 7, 2952, 2953], [8, 13, 1168, 205, 873, 5, 114, 10, 455, 187, 298, 586, 2, 259, 11, 111, 205], [58, 3, 2, 455, 10, 29, 50, 649, 20, 10, 79, 11, 66, 12, 2, 312, 3, 68], [17, 271, 7, 59, 83, 8, 782, 5, 456, 2099, 2, 2954, 3, 2, 356, 4, 214, 10, 874, 49, 492, 5, 237, 129, 4, 493, 5, 7, 875, 42], [2, 355, 3, 783, 2100, 2955, 16, 457, 5, 20, 43, 492, 248, 4, 91, 2, 69, 2956, 11, 141]]\n",
      "IDs back to texts: [\"the queen's messages to those celebrating their 90th birthdays on 21 april\", 'on this shared occasion i send my warm congratulations to you all', 'christmas broadcast 2006', 'the birth of a baby brings great happiness but then the business of growing up begins', \"the queen's christmas broadcast in 2006 focused on understanding between faiths and generations\", 'the broadcast was filmed in southwark cathedral in london where her majesty met schoolchildren working on a nativity collage', 'i have lived long enough to know that things never remain quite the same for very long', 'one of the things that has not changed all that much for me is the celebration of christmas', 'it remains a time when i try to put aside the anxieties of the moment and remember that christ was born to bring peace and tolerance to a troubled world', 'the birth of jesus naturally turns our thoughts to all new born children and what the future holds for them']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(texts) \n",
    "\n",
    "texts2ids = tokenizer.texts_to_sequences(texts)\n",
    "print (\"Texts as IDs:\", texts2ids[:10])\n",
    "ids2texts = tokenizer.sequences_to_texts(texts2ids)\n",
    "print (\"IDs back to texts:\", ids2texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2294\n"
     ]
    }
   ],
   "source": [
    "print(len(texts2ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2): {}, (3, 4): {}, (5, 6): {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {x:{} for x in [(1,2),(3,4),(5,6)]}\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2): {'a': 1, 'b': 2}, (3, 4): {'a': 3, 'b': 4}, (5, 6): {}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[(1,2)] = {'a':1, 'b':2}\n",
    "test[(3,4)] = {'a':3, 'b':4}\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocurrency_example = {\n",
    "        '': {'a':1, 'b':2},\n",
    "        'a': {'b': 1, 'c': 1},\n",
    "        ('a','b'): {'c': 1},\n",
    "        ('b','c'): {'a': 2, 'd': 1},\n",
    "        ('c','d'): {'a': 1, 'b': 1, 'c': 1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toyLM_ngram(ngram_list, ngram_sizes=[2]):\n",
    "    table_list = []\n",
    "    for size in ngram_sizes:\n",
    "        oc_table = {}\n",
    "        for i in range(len(ngram_list)-size):\n",
    "            current_ngram = ngram_list[i:i+size]\n",
    "            for current_ngram in ngram_list:\n",
    "                if current_ngram in oc_table:\n",
    "                    oc_table[current_ngram] += 1\n",
    "                else:\n",
    "                    oc_table[current_ngram] = 1\n",
    "        table_list.append(oc_table)\n",
    "        print(oc_table)\n",
    "    return table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 10, 181: 10, 999: 10, 5: 10, 39: 10, 584: 10, 41: 10, 2094: 10, 2095: 10, 21: 10, 1165: 10, 530: 10}\n",
      "[{2: 10, 181: 10, 999: 10, 5: 10, 39: 10, 584: 10, 41: 10, 2094: 10, 2095: 10, 21: 10, 1165: 10, 530: 10}]\n"
     ]
    }
   ],
   "source": [
    "print(toyLM_ngram(texts2ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams(frase, size=2):\n",
    "    for i in range(size):\n",
    "        frase = [0] + frase\n",
    "    ngrams_list = []\n",
    "    for i in range(len(frase)-size):\n",
    "        ngrams_list.append((tuple(frase[i:i+size]),frase[i+size]))\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 9, 163, 246, 8, 1000, 25, 424, 1166, 5, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "print(texts2ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_ngrams = []\n",
    "for i in texts2ids:\n",
    "    all_ngrams += get_ngrams(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def co_table(lista_ocurrencias):\n",
    "    table = {}\n",
    "    for i in lista_ocurrencias:\n",
    "        if i[0] in table:\n",
    "            if i[1] in table[i[0]]:\n",
    "                table[i[0]][i[1]] += 1\n",
    "            else:\n",
    "                table[i[0]][i[1]] = 1\n",
    "        else:\n",
    "            table[i[0]] = {}\n",
    "            table[i[0]][i[1]] = 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table = co_table(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_toyLM_ngram_a(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for _ in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            return tokenizer.sequences_to_texts([cadena])\n",
    "        else:\n",
    "            new = max(table[context], key=table[context].get)\n",
    "            cadena.append(new)\n",
    "    return tokenizer.sequences_to_texts([cadena])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['younger generations as they are all the more we know that the commonwealth and to']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_toyLM_ngram_a(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_toyLM_ngram_b(table, context='aleatorio', n=15):\n",
    "    if context == 'aleatorio':\n",
    "        tmp = list(table.keys())\n",
    "        context = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "    cadena = list(context)\n",
    "    for i in range(n-len(context)):\n",
    "        context = tuple(cadena[-len(context):])\n",
    "        if context not in table.keys():\n",
    "            return tokenizer.sequences_to_texts([cadena])\n",
    "        else:\n",
    "            lista_tmp = list()\n",
    "            for i in table[context]:\n",
    "                for j in range(table[context][i]):\n",
    "                    lista_tmp.append(i)\n",
    "            new = np.random.randint(len(lista_tmp))\n",
    "            cadena.append(lista_tmp[new])\n",
    "    return tokenizer.sequences_to_texts([cadena])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"warm congratulations to you not from an architect's drawing board but out of date\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_toyLM_ngram_b(table, (424, 1166))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toyLM_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(text, max_seq_length=10):\n",
    "    train_set = dict()\n",
    "    for frase in text:\n",
    "        for word_index in range(len(frase)):\n",
    "            if word_index < max_seq_length:\n",
    "                train_set[tuple(pad_sequences([frase[:word_index]], maxlen=max_seq_length)[0])] = frase[word_index]\n",
    "    return train_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_generate(texts2ids)\n",
    "train_set\n",
    "train_set = dict(list(train_set.items())[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='embedding_8_input'), name='embedding_8_input', description=\"created by layer 'embedding_8_input'\"), but it was called on an input with incompatible shape (None, 10, 5614).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_8\" (type Sequential).\n    \n    Input 0 of layer \"lstm_8\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 10, 5614, 64)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 10, 5614), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PLE\\Repositorio PLN\\main.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=4'>5</a>\u001b[0m     Embedding(\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index), \u001b[39m64\u001b[39m, input_length\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=5'>6</a>\u001b[0m     LSTM(\u001b[39m64\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=6'>7</a>\u001b[0m     Dense(\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=7'>8</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Peter/Desktop/Q6/PLE/Repositorio%20PLN/main.ipynb#ch0000021?line=11'>12</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Peter/Desktop/Q6/PLE/ple/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Peter\\Desktop\\Q6\\PLE\\ple\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_8\" (type Sequential).\n    \n    Input 0 of layer \"lstm_8\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 10, 5614, 64)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 10, 5614), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "x_train = to_categorical(list(train_set.keys()), num_classes=len(tokenizer.word_index))\n",
    "y_train = to_categorical(list(train_set.values()), num_classes=len(tokenizer.word_index))\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(len(tokenizer.word_index), 64, input_length=10),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dense(len(tokenizer.word_index), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16062, 10, 5614)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(x_train, num_classes=len(tokenizer.word_index)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5614)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5614"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5614"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_categorical(list(train_set.values()), num_classes=len(tokenizer.word_index))[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaf8493770d7c647504f8822272130844a080041caffea46428413e90a69177e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('ple': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
